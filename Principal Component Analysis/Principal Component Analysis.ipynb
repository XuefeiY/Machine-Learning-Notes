{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "* __Linearity__\n",
    "\n",
    "Assume the data set to be linear combinations of the variables.\n",
    "\n",
    "* __The importance of mean and covariance__\n",
    "\n",
    "There is no guarantee that the directions of maximum variance will contain good features for discrimination.\n",
    "\n",
    "* __That large variances have important dynamics__\n",
    "\n",
    "Assumes that components with larger variance correspond to interesting dynamics and lower ones correspond to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics\n",
    "\n",
    "$\\bar{X}=\\frac{\\sum{X}}{n}$\n",
    "\n",
    "$Var(X)=\\frac{\\sum_{i=1}^{n}(X-\\bar{X})^2}{n-1}$\n",
    "\n",
    "$Cov(X,Y)=\\frac{\\sum_{i=1}^{n}(X-\\bar{X})(Y-\\bar{Y})}{n-1}$\n",
    "\n",
    "$C=\\begin{bmatrix}cov(x,x) & cov(x,y) & cov(x,z)\\\\cov(y,x) & cov(y,y) & cov(y,z)\\\\cov(z,x) & cov(z,y) & cov(z,z)\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "PCA seeks a linear combination of variables such that the maximum variance is extracted from the variables. It then removes this variance and seeks a second linear combination which explains the maximum proportion of the remaining variance, and so on. This is called the principal axis method and results in orthogonal (uncorrelated) factors. PCA analyzes total (common and unique) variance.\n",
    "\n",
    "\n",
    "\n",
    "### Orthogonal directions with largest variance\n",
    "\n",
    "### Linear subspace with minimal sqaured residuals\n",
    "\n",
    "### Eigendecomposition of the covariance matrix\n",
    "- Let $\\mathbf{A}$ be a square (N×N) matrix with N linearly independent eigenvectors, then A can be factorized as: $\\mathbf{A}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}  $\n",
    "- Eigenvectors of Covariance/Correlation matrix are PC’s. $\\mathbf{Q}$ is the square (N×N) matrix whose ith column is the eigenvector $q_{i}$ of $\\mathbf{A}$ \n",
    "- $\\mathbf{\\Lambda}$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues\n",
    "\n",
    "\n",
    "### Singular Vector Decomposition of the data matrix\n",
    "\n",
    "\n",
    "http://chem-eng.utoronto.ca/~datamining/Presentations/PCA.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* Eigenvectors\n",
    "\n",
    "* Eigenvalues\n",
    "\n",
    "* Factor loadings\n",
    "\n",
    "* PC scores\n",
    "\n",
    "\n",
    "### scree plot\n",
    "\n",
    "A scree plot graphs the amount of variation explained by each component.\n",
    "\n",
    "### interpreting the components\n",
    "\n",
    "### Derivation of PCA (eigenvalue and eigenvector)\n",
    "\n",
    "### PCA in linear regression\n",
    "\n",
    "Identification and elimination of multicolinearities in the data.\n",
    "\n",
    "Reduction in the dimension of the input space leading to fewer parameters and “easier” regression.\n",
    "\n",
    "Related to the last point, the variance of the regression coefficient estimator is minimized by the PCA choice of basis\n",
    "\n",
    "### Pros and Cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
