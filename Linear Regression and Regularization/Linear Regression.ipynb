{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "1. __Linearity and additivity__: 1). There should be a linear relationship between dependent and independent variables. It is also important to check for outliers since linear regression is sensitive to outlier effects. 2). The impact of change in independent variables values should have additive impact on dependent variables.\n",
    "\n",
    "  __How to diagnose__: The linearity assumption can best be tested with scatter plots.\n",
    "  \n",
    "  __How to fix__: Consider applying a nonlinear transformation to the dependent and/or independent variables. Another possibility to consider is adding another regressor that is a nonlinear function of one of the other variables.\n",
    "      \n",
    "2. __Statistical independence of errors__: The residuals are independent, in particular, no correlation between consecutive errors in the case of time series data.\n",
    "\n",
    "   __How to diagnose__: To test for non-time-series violations of independence, you can look at plots of the residuals versus independent variables. The residuals should be randomly and symmetrically distributed around zero.\n",
    "   \n",
    "   __How to fix__: It could be due to a violation of the linearity assumption or due to bias that is explainable by omitted variables (say, interaction terms or dummies for identifiable conditions).\n",
    "\n",
    "3. __Normality of residuals__: Distribution of residuals should be normal distributed. \n",
    "\n",
    "  __How to diagnose__: This assumption can best be checked with a histogram or a Q-Q plot. \n",
    "\n",
    "  __How to fix__: violations of normality often arise either because (a) the distributions of the dependent and/or independent variables are themselves significantly non-normal, and/or (b) the linearity assumption is violated. In such cases, a nonlinear transformation of variables might cure both problems. \n",
    "\n",
    "  __Note__:  1). The dependent and independent variables in a regression model do not need to be normally distributed by themselves--only the prediction errors need to be normally distributed. But if the distributions of some of the variables that are random are extremely asymmetric or long-tailed, it may be hard to fit them into a linear model whose errors will be normally distributed. \n",
    "  2). $\\hat{y_i}=x_i\\beta+\\hat{\\epsilon_i}$, $\\hat{\\epsilon_i}$ is an observation here and $\\hat{\\epsilon_1}...\\hat{\\epsilon_n}$ should follow a normal distribution.\n",
    "  \n",
    "4. __Homoscedasticity__: Variance of errors should be constant versus, a. Time, b. The predictions, c. Independent variable values. \n",
    "\n",
    "  __How to diagnose__: Look at a plot of residuals versus predicted values and, in the case of time series data, a plot of residuals versus time. To be really thorough, you should also generate plots of residuals versus independent variables to look for consistency there as well. \n",
    "  \n",
    "  __How to fix__: If the dependent variable is strictly positive and if the residual-versus-predicted plot shows that the size of the errors is proportional to the size of the predictions, a log transformation applied to the dependent variable may be appropriate. \n",
    "  \n",
    "  __Note__: $y_i=x_i\\beta+\\epsilon_i$, $\\epsilon_i$ is a reandom variable here and has a variance $\\sigma^2_i$\n",
    "\n",
    "5. __ No multicollinearity__:  linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs when the independent variables are too highly correlated with each other.\n",
    "\n",
    "  __How to diagnose__:  \n",
    "   * Correlation matrix \n",
    "   * Tolerance：With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.\n",
    "   $$T_k=1 - R_k^2$$\n",
    "   * Variance Inflation Factor (VIF): If the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "   $$VIF_k = {1}/{(1 - R_k^2)}$$ where $R^2_k$  is the $R^2$-value obtained by regressing the kth predictor on the remaining predictors.  \n",
    "\n",
    "  __How to fix__: Centering the data (that is deducting the mean of the variable from each score) might help to solve the problem. The simplest way to address the problem is to remove independent variables with high VIF values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "* __$R^2$__: A common way to summarize how well a linear regression model fits the data is via the coefficient of determination or $R^2$.. It is the proportion of variation in the forecasr variable that is explained by the regression model. It can be calculated as:$$R^2 = \\frac{\\sum(\\hat{y_i}-\\bar{y})^2}{\\sum(y_i-\\bar{y})^2}$$\n",
    "\n",
    "  The $R^2$ value is commonly used, often incorrectly, in forecasting. There are no set rules of what a good $R^2$ value is and typical values of $R^2$ depend on the type of data used. Validating a model’s out-of-sample forecasting performance is much better than measuring the in-sample R^2 value.\n",
    "  \n",
    "* __Adjusted $R^2$__:$$R_{adj}^2=1-[\\frac{(1-R^2)(n-1)}{n-k-1}]$$\n",
    " where: n is the number of points in your data sample, k is the number of variables in your model, excluding the constant.\n",
    "  \n",
    "* __F-Test__:\n",
    "\n",
    "* __MSE__:\n",
    "\n",
    "http://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Algorithms Used to Estimate the Coefficients\n",
    "* __Simple Linear Regression__\n",
    "\n",
    "When there is a single input variable (x), the method is referred to as simple linear regression. \n",
    "\n",
    "* __Ordinary Least Squares__\n",
    "\n",
    "When we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients. The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals.\n",
    "\n",
    "* __Gradient Descent__\n",
    "\n",
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "* __Regularization__\n",
    "\n",
    "These seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types and Coding\n",
    "* Numerical\n",
    "* Categorical: one-hot-encodeing, integer encoding\n",
    "* Ordinal: a natural ordering between categories\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/lehre/ml-07w/skript/ml-2up-01-linearregression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Linear Regression\n",
    "* Linear Assumption (log transform)\n",
    "* Remove Noise (remove outlier)\n",
    "* Remove Collinearity (overfit)\n",
    "* Gaussian Distributions (log or Boxcox transform)\n",
    "* Rescale Inputs (standardization or normalization)\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/linear-regression-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression  Diagnostics\n",
    "https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    "### Ridge Regression\n",
    "Ridge Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} \\theta_i^2$\n",
    "\n",
    "### Lasso Regression\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
    "of the weight vector instead of half the square of the ℓ2 norm.\n",
    "\n",
    "Lasso Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} |\\theta_i|$\n",
    "\n",
    "### Elastic Net\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression.\n",
    "\n",
    "Elastic Net cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + r\\alpha\\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha\\sum_{i=1}^{n}\\theta_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models in Python\n",
    "* __scipy.stats.linregress__ only handles the case of a single explanatory variable with specialized code and calculates a few extra statistics.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "* __statsmodels.OLS__ is a generic linear model (OLS) estimation class. It doesn't prespecify what the explanatory variables are and can handle any multivariate array of explanatory variables, or formulas and pandas DataFrames. It not only returns the estimated parameters, but also a large set of results staistics and methods for statistical inference and prediction.\n",
    "\n",
    "* __sklearn.linear_model__\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Assumption: http://people.duke.edu/~rnau/testing.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
