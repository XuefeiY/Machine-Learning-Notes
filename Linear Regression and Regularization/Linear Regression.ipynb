{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "1. __Linearity and additivity__: 1). There should be a linear relationship between dependent and independent variables. It is also important to check for outliers since linear regression is sensitive to outlier effects. 2). The impact of change in independent variables values should have additive impact on dependent variables.\n",
    "\n",
    "  __How to diagnose__: The linearity assumption can best be tested with scatter plots.\n",
    "  \n",
    "  __How to fix__: Consider applying a nonlinear transformation to the dependent and/or independent variables. Another possibility to consider is adding another regressor that is a nonlinear function of one of the other variables.\n",
    "      \n",
    "2. __Statistical independence of errors__: The residuals are independent, in particular, no correlation between consecutive errors in the case of time series data.\n",
    "\n",
    "   __How to diagnose__: To test for non-time-series violations of independence, you can look at plots of the residuals versus independent variables. The residuals should be randomly and symmetrically distributed around zero.\n",
    "   \n",
    "   __How to fix__: It could be due to a violation of the linearity assumption or due to bias that is explainable by omitted variables (say, interaction terms or dummies for identifiable conditions).\n",
    "\n",
    "3. __Normality of residuals__: Distribution of residuals should be normal distributed. \n",
    "\n",
    "  __How to diagnose__: This assumption can best be checked with a histogram or a Q-Q plot. \n",
    "\n",
    "  __How to fix__: violations of normality often arise either because (a) the distributions of the dependent and/or independent variables are themselves significantly non-normal, and/or (b) the linearity assumption is violated. In such cases, a nonlinear transformation of variables might cure both problems. \n",
    "\n",
    "  __Note__:  1). The dependent and independent variables in a regression model do not need to be normally distributed by themselves--only the prediction errors need to be normally distributed. But if the distributions of some of the variables that are random are extremely asymmetric or long-tailed, it may be hard to fit them into a linear model whose errors will be normally distributed. \n",
    "  2). $\\hat{y_i}=x_i\\beta+\\hat{\\epsilon_i}$, $\\hat{\\epsilon_i}$ is an observation here and $\\hat{\\epsilon_1}...\\hat{\\epsilon_n}$ should follow a normal distribution.\n",
    "  \n",
    "4. __Homoscedasticity__: Variance of errors should be constant versus, a. Time, b. The predictions, c. Independent variable values. \n",
    "\n",
    "  __How to diagnose__: Look at a plot of residuals versus predicted values and, in the case of time series data, a plot of residuals versus time. To be really thorough, you should also generate plots of residuals versus independent variables to look for consistency there as well. \n",
    "  \n",
    "  __How to fix__: If the dependent variable is strictly positive and if the residual-versus-predicted plot shows that the size of the errors is proportional to the size of the predictions, a log transformation applied to the dependent variable may be appropriate. \n",
    "  \n",
    "  __Note__: $y_i=x_i\\beta+\\epsilon_i$, $\\epsilon_i$ is a reandom variable here and has a variance $\\sigma^2_i$\n",
    "\n",
    "5. __ No multicollinearity__:  linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs when the independent variables are too highly correlated with each other.\n",
    "\n",
    "  __How to diagnose__:  \n",
    "   * Correlation matrix \n",
    "   * Tolerance：With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.\n",
    "   $$T_k=1 - R_k^2$$\n",
    "   * Variance Inflation Factor (VIF): If the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "   $$VIF_k = {1}/{(1 - R_k^2)}$$ where $R^2_k$  is the $R^2$-value obtained by regressing the kth predictor on the remaining predictors.  \n",
    "\n",
    "  __How to fix__: Centering the data (that is deducting the mean of the variable from each score) might help to solve the problem. The simplest way to address the problem is to remove independent variables with high VIF values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Variance (ANOVA)\n",
    "When there is no association between Y and X, the best predictor of each observation is $\\bar{y}=\\hat{\\beta_0}$. In this case, the toal variation can be denoted as $TSS=\\sum(y_i-\\bar{y})^2$, the __Total Sum of Squares__.\n",
    "\n",
    "\n",
    "When there is an association between Y and X, the best predictor of each observation is $\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i$. In this case, the error variation can be denoted as $SSE=\\sum(y_i-\\hat{y_i})^2$, the __Error Sum of Squares__.\n",
    "\n",
    "\n",
    "The difference between TSS and SSE is the variation \"explained\" by the regression of Y on X. It represents the difference between the fitted values and the mean: $SSR=\\sum(\\hat{y_i}-\\bar{y})^2$, the __Regression Sum of Squares__.\n",
    "\n",
    "\n",
    " The relationship among them is: $TSS=SSE+SSR$, $\\sum(y_i-\\bar{y})^2=\\sum(y_i-\\hat{y_i})^2+\\sum(\\hat{y_i}-\\bar{y})^2$\n",
    " \n",
    "\n",
    "A common way to summarize how well a linear regression model fits the data is via the coefficient of determination or $R^2$. It is the proportion of variation in the forecast variable that is explained by the regression model. It can be calculated as:$$R^2 = \\frac{\\sum(\\hat{y_i}-\\bar{y})^2}{\\sum(y_i-\\bar{y})^2}=\\frac{SSR}{TSS}$$\n",
    "\n",
    "The __$R^2$ value__ is commonly used, often incorrectly, in forecasting. There are no set rules of what a good $R^2$ value is and typical values of $R^2$ depend on the type of data used. Validating a model’s out-of-sample forecasting performance is much better than measuring the in-sample $R^2$ value.\n",
    "\n",
    "  \n",
    "The __Adjusted $R^2$__ is a modified version of $R^2$ that has been adjusted for the number of predictors in the model. The adjusted $R^2$ increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted $R^2$ can be negative, but it’s usually not. It is always lower than the $R^2$. \n",
    "$$R_{adj}^2=1-[\\frac{(1-R^2)(n-1)}{n-k-1}]$$\n",
    "    where: \n",
    "   * n is the number of points in your data sample  \n",
    "   * k is the number of variables in your model, excluding the constant.\n",
    "\n",
    "__Mean Squared Error (MSE)__ measures the average of the squares of the errors or deviations. If $\\hat{y}$ is a vector of n predictions, and $y$ is the vector of observed values corresponding to the inputs to the function which generated the predictions, then MSE of the predictor can be estimated by$$MSE=\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i}-y_i)^2$$\n",
    "\n",
    "\n",
    "__ANOVA__ calculations are displayed in an analysis of variance table, which has the following format for multiple linear regression:\n",
    "\n",
    "| Source | Degrees of Freedom | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sum of squares&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mean Square&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$F_{obs}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P-value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |\n",
    "| :----- | :-----: | :----- | :----- | :-----: | :-----: |\n",
    "| Model | p | $SSR=\\sum(\\hat{y_i}-\\bar{y})^2$ | $MSR=\\frac{SSR}{p}$ | $F=\\frac{MSR}{MSE}$ | $P(F_{p, n-p-1} \\ge F_{obs})$ |\n",
    "| Error | n-p-1 | $SSE=\\sum(y_i-\\hat{y_i})^2$ | $MSE=\\frac{SSE}{n-p-1}$ | &nbsp; | &nbsp; |\n",
    "| Total | n-1 | $TSS=\\sum(y_i-\\bar{y})^2$ | &nbsp; | &nbsp; | &nbsp; |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses Tests for the Slopes\n",
    "__F test: testing all slope parameters equal 0__\n",
    "\n",
    "For a multiple regression model with intercept, we want to test the following null hypothesis and alternative hypothesis:\n",
    "\n",
    "$H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0$\n",
    " \n",
    "$H1: \\beta_j \\neq 0$, for at least one value of $j$\n",
    "\n",
    "This test is known as the overall __F-test for regression__. The test statistic should be $F=\\frac{MSR}{MSE}$ and we should find a $(1-\\alpha)100\\%$ confidence interval $I$ for $(p, n-p-1)$ degrees of freedom using an F table. Accept the null hypothesus if $F \\in I$, reject if $F \\notin I$.\n",
    "   \n",
    "__T test: testing one slope parameter is 0__\n",
    "\n",
    "The  test is used to check the significance of individual regression coefficients in the multiple linear regression model. The hypothesis statements to test the significance of a particular regression coefficient $\\beta_j$ are:\n",
    "\n",
    "$H_0: \\beta_j = 0$\n",
    " \n",
    "$H1: \\beta_j \\neq  0$\n",
    "\n",
    "The test statistic for this test is based on the t distribution: $T_0 = \\frac{\\hat{\\beta_j}}{se(\\hat{\\beta_j})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vairable Selection Criterion\n",
    "__Akaike's Information Criterion (AIC)__\n",
    "\n",
    "A simple formula for the calculation of the AIC in the OLS framework is:\n",
    "$$AIC=n*log(\\frac{SSE}{n})+2k$$\n",
    "Where SSE means Sum of Squared Errors $\\sum(y_i-\\hat{y_i})^2$, n is the sample size, and k is the number of predictors in the model plus one for the intercept. \n",
    "\n",
    "__Bayes Information Criterion (BIC)__\n",
    "\n",
    "A simple formula for the calculation of the BIC in the OLS framework is:\n",
    "$$BIC=n*log(\\frac{SSE}{n})+klog(n)$$\n",
    "Larger models will fit better and so have smaller SSE but use more parameters. Thus the best choice of model will balance fit with model size. BIC penalizes larger models more heavily and so will tend to prefer smaller models in comparison to AIC. AIC and BIC can be used as selection criteria for other types of model too.\n",
    "\n",
    "\n",
    "http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Algorithms Used to Estimate the Coefficients\n",
    "* __Simple Linear Regression__\n",
    "\n",
    "When there is a single input variable (x), the method is referred to as simple linear regression. \n",
    "\n",
    "* __Ordinary Least Squares__\n",
    "\n",
    "When we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients. The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals.\n",
    "\n",
    "* __Gradient Descent__\n",
    "\n",
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "* __Regularization__\n",
    "\n",
    "These seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Linear Regression\n",
    "* Linear Assumption (log transform)\n",
    "* Remove Noise (remove outlier)\n",
    "* Remove Collinearity (overfit)\n",
    "* Gaussian Distributions (log or Boxcox transform)\n",
    "* Rescale Inputs (standardization or normalization)\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/linear-regression-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression  Diagnostics\n",
    "https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    "### Ridge Regression\n",
    "Ridge Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} \\theta_i^2$\n",
    "\n",
    "### Lasso Regression\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
    "of the weight vector instead of half the square of the ℓ2 norm.\n",
    "\n",
    "Lasso Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} |\\theta_i|$\n",
    "\n",
    "### Elastic Net\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression.\n",
    "\n",
    "Elastic Net cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + r\\alpha\\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha\\sum_{i=1}^{n}\\theta_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models in Python\n",
    "* __scipy.stats.linregress__ only handles the case of a single explanatory variable with specialized code and calculates a few extra statistics.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "* __statsmodels.OLS__ is a generic linear model (OLS) estimation class. It doesn't prespecify what the explanatory variables are and can handle any multivariate array of explanatory variables, or formulas and pandas DataFrames. It not only returns the estimated parameters, but also a large set of results staistics and methods for statistical inference and prediction.\n",
    "\n",
    "* __sklearn.linear_model__\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Assumption: http://people.duke.edu/~rnau/testing.htm\n",
    "\n",
    "ANOVA: http://www.stat.ufl.edu/~winner/statnotescomp/regression.pdf\n",
    "\n",
    "ANOVA Table: http://www.stat.yale.edu/Courses/1997-98/101/anovareg.htm\n",
    "\n",
    "Hypothesis Test: http://reliawiki.org/index.php/Multiple_Linear_Regression_Analysis#Test_on_Individual_Regression_Coefficients_.28t__Test.29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
