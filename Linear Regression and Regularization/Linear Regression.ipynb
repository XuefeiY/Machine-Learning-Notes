{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "##  Learning Algorithms Used to Estimate the Coefficients\n",
    "* __Simple Linear Regression__\n",
    "\n",
    "When there is a single input variable (x), the method is referred to as simple linear regression. \n",
    "\n",
    "* __Ordinary Least Squares__\n",
    "\n",
    "When we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients. The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals.\n",
    "\n",
    "* __Gradient Descent__\n",
    "\n",
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "* __Regularization__\n",
    "\n",
    "These seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model).\n",
    "\n",
    "\n",
    "## Prepare Data for Linear Regression\n",
    "* Linear Assumption (log transform)\n",
    "* Remove Noise (remove outlier)\n",
    "* Remove Collinearity (overfit)\n",
    "* Gaussian Distributions (log or Boxcox transform)\n",
    "* Rescale Inputs (standardization or normalization)\n",
    "\n",
    "https://machinelearningmastery.com/linear-regression-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types and Coding\n",
    "* Numerical\n",
    "* Categorical: one-hot-encodeing, integer encoding\n",
    "* Ordinal: a natural ordering between categories\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/lehre/ml-07w/skript/ml-2up-01-linearregression.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumtion\n",
    "* __Linearity & Additive__: There should be a linear relationship between dependent and independent variables and the impact of change in independent variables values should have additive impact on dependent variables.\n",
    "* __Normality of error distribution__: Distribution of residuals should be normal distributed.\n",
    "* __Homoscedasticity__: Variance of errors should be constant versus, a. Time, b. The predictions, c. Independent variable values\n",
    "* __Statistical independence of errors__: The residuals should not have any correlation among themselves.\n",
    "\n",
    "https://www.dezyre.com/data-science-in-r-programming-tutorial/linear-regression-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "* __$R^2$__: A measure of how well observed outcomes are replicated by the model, as the proportion of total variation of\n",
    "outcomes explained by the model.\n",
    "\n",
    "http://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression  Diagnostics\n",
    "https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity\n",
    "The Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression. Inspect the factors for each predictor variable, if the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "$$VIF = \\frac{1}{(1 - R^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    "### Ridge Regression\n",
    "Ridge Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} \\theta_i^2$\n",
    "\n",
    "### Lasso Regression\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
    "of the weight vector instead of half the square of the ℓ2 norm.\n",
    "\n",
    "Lasso Regression cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha\\sum_{i=1}^{n} |\\theta_i|$\n",
    "\n",
    "### Elastic Net\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
    "Regression, and when r = 1, it is equivalent to Lasso Regression.\n",
    "\n",
    "Elastic Net cost function\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + r\\alpha\\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha\\sum_{i=1}^{n}\\theta_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models in Python\n",
    "* __scipy.stats.linregress__ only handles the case of a single explanatory variable with specialized code and calculates a few extra statistics.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "\n",
    "* __statsmodels.OLS__ is a generic linear model (OLS) estimation class. It doesn't prespecify what the explanatory variables are and can handle any multivariate array of explanatory variables, or formulas and pandas DataFrames. It not only returns the estimated parameters, but also a large set of results staistics and methods for statistical inference and prediction.\n",
    "\n",
    "* __sklearn.linear_model__\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
