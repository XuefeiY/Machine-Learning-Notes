{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification and Regression Trees (CART)\n",
    "Trees provide alternative ways of modeling nonlinear relationships by carving out rectangular regions in the covariate space.\n",
    "* Response variables can be categorical or quantitative.\n",
    "* Yields a set of interpretable decision rules.\n",
    "* Predictive ability is mediocre, but can be improved with ideas of resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Metrics for Measuring Best Split\n",
    "For categorical target variable:\n",
    "* Gini Index\n",
    "* Chi Square\n",
    "* Information Gain\n",
    "\n",
    "For continuous target variables:\n",
    "* Reduction in Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Tree\n",
    "Intuitively, we want to choose $R_1$, . . . , $R_J$ in the covariate space to minimize error:\n",
    "$$RSS = \\sum_{j=1}^{J}\\sum_{i\\in R_j}(y_i - \\hat{y}_{R_j})^2$$\n",
    "\n",
    "where the predicted value for any observation in region $R_j$ is $\\hat{y}_{R_j}=\\frac{1}{|R_j|}\\sum_{i \\in R_j}y_i$\n",
    "\n",
    "__Greedy approach__\n",
    "* Grow the tree by recursive binary splitting\n",
    "* Prune back the tree\n",
    "\n",
    "__Stop Criterion__\n",
    "* number of observations in a node has reached a minimum\n",
    "* depth of tree has reached a maximum\n",
    "* grow until no further splits can reduce RSS by some amount\n",
    "\n",
    "__Cost-Complexity Pruning__\n",
    "$$C(T) = \\sum_{m=1}^{|T|}\\sum_{i \\in R_m}(y_i - \\hat{y}_{R_m})^2 + \\alpha|T|$$\n",
    "$\\alpha$ is a tuning parameter that controls for the complexity of the model.\n",
    "* $\\alpha$ = 0 implies the full tree\n",
    "* Larger $\\alpha$ implies higher penalty for complexity of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Tree\n",
    "$\\hat{y}_i$ for all $i \\in R_j$ is most commonly occuring class of training observations in $R_j$\n",
    "$$\\hat{y}_{R_j} = \\mathop{\\arg\\max}_{k}\\hat{p}_{jk}$$\n",
    "where $\\hat{p}_{jk}$ is the proportion of training observations in the $R_j$\n",
    "\n",
    "No longer want to minimize RSS, but instead to minimize\n",
    "* classification error rate\n",
    "$$E=\\sum_{j=1}^{J}|R_j|(1-\\mathop{\\max}_{k}(\\hat{p}_{jk}))$$\n",
    "* Gini index\n",
    "$$G = \\sum_{j=1}^{J}|R_j|\\sum_{k=1}^{K}\\hat{p}_{jk}(1-\\hat{p}_{jk})$$\n",
    "Encourages higher node purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variable Importacec\n",
    "Various variable importance measures can help shed light on the usefulness in each of the predictors in the splitting process.\n",
    "\n",
    "Classification trees: reduction in Gini index due to splits over a given predictor\n",
    "\n",
    "Regression trees: reduction in RSS due to splits over a given predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pros and Cons\n",
    "### Advantages\n",
    "1. Easy to Understand\n",
    "2. Useful in Data exploration: identify most significant variables and relation between two or more variables\n",
    "3. Less data cleaning required: it is not influenced by outliers and missing values to a fair degree\n",
    "4. Data type is not a constraint: both numerical and categorical vars\n",
    "5. Non Parametric Method: no assumption about the space distribution and the classifier structure\n",
    "\n",
    "### Disadvantages\n",
    "1. Over fitting: solved by pruning\n",
    "2. No fit for continuous variables: loose information when it categorizes variables in different categories\n",
    "\n",
    "### How to deal with overfitting?\n",
    "\n",
    "Setting Constraints on Tree Size\n",
    "\n",
    "* Minimum samples for a node split\n",
    "* Minimum samples for a terminal node (leaf)\n",
    "* Maximum depth of tree (vertical depth)\n",
    "* Maximum number of terminal nodes\n",
    "* Maximum features to consider for split\n",
    "\n",
    "Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trees vs. Other Methods\n",
    "\n",
    "* k-nearest neighbors\n",
    "  * Both produce simple predictions (averages/maximally occurring) based on “neighborhoods” in the predictor space. \n",
    "  * However, decision trees use adaptive neighborhoods.\n",
    "\n",
    "* linear regression\n",
    "Regression trees are like fitting linear regression models with a bunch of indicators\n",
    "$$f(x) = \\sum_{j=1}^{J}\\beta_j\\mathbb{1}\\{x \\in R_j\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
